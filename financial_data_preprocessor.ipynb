{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16fc60c0",
   "metadata": {},
   "source": [
    "#### Financial Market Data Pre-Processor\n",
    "\n",
    "##### Software Pre-requisites:\n",
    "```\n",
    "pip install pandas\n",
    "pip install finnhub-python\n",
    "```\n",
    "\n",
    "##### Running instructions \n",
    "- Press `Run All` Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ab90cdc-f2b3-41ba-a4a4-0eff99765e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import finnhub\n",
    "import pandas as pd\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# dates for request parameters\n",
    "start_time = time.time()\n",
    "date_today = date.today()\n",
    "today_date = date_today.strftime('%Y-%m-%d')\n",
    "current_year = str(date_today.year)\n",
    "\n",
    "from_time_unix = int(time.mktime((date_today - timedelta(weeks = 52)).timetuple()))\n",
    "to_time_unix = int(time.mktime(date_today.timetuple()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ded1bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finnhub clients setup - https://finnhub.io/docs/api/\n",
    "def generate_finnhub_clients():\n",
    "    finnhub_keys = ['Y2Q0b3FpYWFkM2k5OGpodTJwZ2djZDRvcWlhYWQzaTk4amh1MnBoMA==','Y2UzbWQyYWFkM2kxaDJuN24xODBjZTNtZDJhYWQzaTFoMm43bjE4Zw==','Y2R0ZG1yMmFkM2k0MXY3aG9nM2djZHRkbXIyYWQzaTQxdjdob2c0MA==',\n",
    "        'Y2U1NmViaWFkM2lmZHZ0aHQzcjBjZTU2ZWJpYWQzaWZkdnRodDNyZw==','Y2U0dTFiMmFkM2llMTg4dGY4bmdjZTR1MWIyYWQzaWUxODh0ZjhvMA==','Y2UzbWU2MmFkM2kxaDJuN24xc2djZTNtZTYyYWQzaTFoMm43bjF0MA==',\n",
    "        'Y2UzbWtiYWFkM2kxaDJuN240ZGdjZTNta2JhYWQzaTFoMm43bjRlMA==','Y2U1NjlxMmFkM2lmZHZ0aHQwZTBjZTU2OXEyYWQzaWZkdnRodDBlZw==','Y2UzbXYxYWFkM2kxaDJuN244dWdjZTNtdjFhYWQzaTFoMm43bjh2MA==',\n",
    "        'Y2U0dHE5YWFkM2llMTg4dGY0YzBjZTR0cTlhYWQzaWUxODh0ZjRjZw==','Y2U0dTQyaWFkM2llMTg4dGZhYWdjZTR1NDJpYWQzaWUxODh0ZmFiMA==','Y2U1NmQ2cWFkM2lmZHZ0aHQzM2djZTU2ZDZxYWQzaWZkdnRodDM0MA==']\n",
    "\n",
    "    finnhub_client_list = []\n",
    "\n",
    "    for key in finnhub_keys:\n",
    "        finnhub_client_list.append(finnhub.Client(api_key=base64.b64decode(key).decode()))\n",
    "\n",
    "    return finnhub_client_list\n",
    "\n",
    "finnhub_clients = generate_finnhub_clients()\n",
    "client_num = 0\n",
    "\n",
    "def get_finhub_client():\n",
    "    \"\"\" returns a finnhub client to perform requests to gather financial data \"\"\"\n",
    "\n",
    "    global client_num\n",
    "    if client_num >= len(finnhub_clients) - 1:\n",
    "        client_num = 0\n",
    "    else:\n",
    "        client_num+=1\n",
    "\n",
    "    return finnhub_clients[client_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "061aaf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_path(csv_path):\n",
    "    csv_file_path = Path(csv_path)\n",
    "    csv_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return csv_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "376e81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_candlestick_data_frame(ticker_symbol):\n",
    "    \n",
    "    candle_response = get_finhub_client().stock_candles(symbol=ticker_symbol, resolution='D', _from=from_time_unix, to=to_time_unix)\n",
    "\n",
    "    candles = pd.json_normalize(candle_response)[['t', 'o', 'c', 'h', 'l', 'v']]\n",
    "    \n",
    "    candles_df = pd.DataFrame(columns=['date','unix_time', 'open', 'close', 'high', 'low', 'volume'])\n",
    "    \n",
    "    candles_df['unix_time'] = candles['t'][0]\n",
    "    candles_df['open'] = candles['o'][0]\n",
    "    candles_df['close'] = candles['c'][0]\n",
    "    candles_df['high'] = candles['h'][0]\n",
    "    candles_df['low'] = candles['l'][0]\n",
    "    candles_df['volume'] = candles['v'][0]\n",
    "\n",
    "    candles_df.sort_values(by=['unix_time'], ascending=False, inplace=True)\n",
    "    candles_df.drop_duplicates(subset=['unix_time'], keep='last', inplace=True)\n",
    "\n",
    "    candles_df['date'] = pd.to_datetime(candles_df['unix_time'],unit='s').astype(str)    \n",
    "\n",
    "    return candles_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aecd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_candlestick_data(ticker_symbol):\n",
    "    \"\"\" populates the daily candlestick data for the given stock into the file: 'data/candlestick_data.csv' \"\"\"\n",
    "\n",
    "    candlestick_df = retrieve_candlestick_data_frame(ticker_symbol)\n",
    "\n",
    "    if not(candlestick_df.empty):\n",
    "\n",
    "        candlestick_df.insert(0,'symbol', ticker_symbol)\n",
    "\n",
    "        csv_file_path = create_csv_path(\"data/candlestick_data.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, candlestick_df], axis=0).drop_duplicates(subset=['symbol', 'unix_time'], keep='last').sort_values(by=['symbol', 'unix_time'], ascending=[True, False])\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                candlestick_df.sort_values(by=['symbol', 'unix_time'], ascending=[True, False]).to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating candlestick data for {ticker_symbol}, error: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79708c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_technical_data_frame(ticker_symbol):\n",
    "    \n",
    "    bband_response = get_finhub_client().technical_indicator(symbol=ticker_symbol, resolution='D', _from=from_time_unix, to=to_time_unix, indicator='bbands', indicator_fields={\"timeperiod\": 20})\n",
    "    tech_data = pd.json_normalize(bband_response)[['t', 'o', 'c', 'h', 'l', 'v', 'lowerband', 'middleband', 'upperband']]\n",
    "    \n",
    "    technical_df = pd.DataFrame(columns=['date','unix_time', 'open', 'close', 'high', 'low', 'volume', 'lowerband', 'middleband', 'upperband'])\n",
    "    \n",
    "    technical_df['unix_time'] = tech_data['t'][0]\n",
    "    technical_df['open'] = tech_data['o'][0]\n",
    "    technical_df['close'] = tech_data['c'][0]\n",
    "    technical_df['high'] = tech_data['h'][0]\n",
    "    technical_df['low'] = tech_data['l'][0]\n",
    "    technical_df['volume'] = tech_data['v'][0]\n",
    "    technical_df['lowerband'] = tech_data['lowerband'][0]\n",
    "    technical_df['middleband'] = tech_data['middleband'][0]\n",
    "    technical_df['upperband'] = tech_data['upperband'][0]\n",
    "\n",
    "    technical_df.sort_values(by=['unix_time'], ascending=False, inplace=True)\n",
    "    technical_df.drop_duplicates(subset=['unix_time'], keep='last', inplace=True)\n",
    "\n",
    "    technical_df = technical_df[technical_df['upperband'] > 0]\n",
    "\n",
    "    technical_df['date'] = pd.to_datetime(technical_df['unix_time'],unit='s').astype(str)\n",
    "\n",
    "    return technical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f206d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_technical_data(ticker_symbol):\n",
    "    \"\"\" populates the daily technical indicator data for the given stock into the file: 'data/technical_indicators.csv' \"\"\"\n",
    "\n",
    "    stock_technical_df = retrieve_technical_data_frame(ticker_symbol)\n",
    "\n",
    "    if not(stock_technical_df.empty):\n",
    "\n",
    "        stock_technical_df.insert(0,'symbol', ticker_symbol)\n",
    "\n",
    "        csv_file_path = create_csv_path(\"data/technical_indicators.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, stock_technical_df], axis=0).drop_duplicates(subset=['symbol', 'unix_time'], keep='last').sort_values(by=['symbol', 'unix_time'], ascending=[True, False])\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                stock_technical_df.sort_values(by=['symbol', 'unix_time'], ascending=[True, False]).to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating technical indicator data for {ticker_symbol}, error: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38cdab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_social_columns = {\"mention\": \"mention_twitter\", \"positiveScore\": \"positiveScore_twitter\",\"negativeScore\": \"negativeScore_twitter\",\n",
    "    \"positiveMention\": \"positiveMention_twitter\",\"negativeMention\": \"negativeMention_twitter\",\"score\": \"score_twitter\"}\n",
    "\n",
    "def retrieve_social_sentiment_data_frame(ticker_symbol):\n",
    "\n",
    "    social_response = get_finhub_client().stock_social_sentiment(ticker_symbol)\n",
    "\n",
    "    twitter_social_df = pd.json_normalize(social_response, record_path='twitter')\n",
    "    reddit_social_df = pd.json_normalize(social_response, record_path='reddit')\n",
    "\n",
    "    try:\n",
    "        if 'atTime' in reddit_social_df:\n",
    "            social_df = twitter_social_df.merge(reddit_social_df, how='left', on=['atTime'], suffixes=('', '_reddit')).fillna(0).rename(columns=renamed_social_columns)\n",
    "\n",
    "        else:\n",
    "            social_df = twitter_social_df.fillna(0).rename(columns=renamed_social_columns)\n",
    "\n",
    "    except Exception as error:\n",
    "        print(f\"Error while gathering social sentiment for {ticker_symbol} error: {error}\")\n",
    "        social_df = twitter_social_df.fillna(0).rename(columns=renamed_social_columns)\n",
    "\n",
    "    return social_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1242f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_social_sentiment(ticker_symbol):\n",
    "    \"\"\" populates the social sentiment for stocks on Reddit and Twitter for the given stock into the file: 'data/social_media_sentiment.csv' \"\"\"\n",
    "\n",
    "    social_df = retrieve_social_sentiment_data_frame(ticker_symbol)\n",
    "    if not(social_df.empty):\n",
    "        \n",
    "        social_df.insert(0,'symbol', ticker_symbol)\n",
    "        csv_file_path = create_csv_path(\"data/social_media_sentiment.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, social_df], axis=0).drop_duplicates(keep='last').sort_values(by=['symbol', 'atTime'], ascending=[True, False]).fillna(0)\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                social_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating social sentiment data for {ticker_symbol}, error: {error}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af1ddb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "insider_trans_cols =['symbol','share','change','transactionDate','transactionCode','transactionPrice','name','filingDate','id']\n",
    "\n",
    "def populate_insider_transactions(ticker_symbol):\n",
    "    \"\"\" populates insider transactions into file 'data/insider_transactions.csv' \"\"\"\n",
    "\n",
    "    insider_transactions_response = get_finhub_client().stock_insider_transactions(ticker_symbol)\n",
    "\n",
    "    insider_trans_df = pd.json_normalize(insider_transactions_response, record_path='data')\n",
    "\n",
    "    if not insider_trans_df.empty:\n",
    "        csv_file_path = create_csv_path(\"data/insider_transactions.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, insider_trans_df], axis=0).drop_duplicates().sort_values(by=['symbol','transactionDate'], ascending=[True, False])[insider_trans_cols]\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                insider_trans_df.sort_values(by=['symbol', 'transactionDate'], ascending=[True, False])[insider_trans_cols].to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating insider transactions data for {ticker_symbol}, error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8afbb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_cols =['symbol','period','actual','estimate','surprise','surprisePercent']\n",
    "\n",
    "def populate_company_surprise_earnings(ticker_symbol):\n",
    "    \"\"\" populates company surprise earnings into file 'data/surprise_earnings.csv' \"\"\"\n",
    "\n",
    "    earnings_response = get_finhub_client().company_earnings(ticker_symbol)\n",
    "\n",
    "    earnings_df = pd.json_normalize(earnings_response)\n",
    "\n",
    "    if not earnings_df.empty:\n",
    "        csv_file_path = create_csv_path(\"data/surprise_earnings.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, earnings_df], axis=0).drop_duplicates(subset=['symbol', 'period']).sort_values(by=['symbol','period'], ascending=[True, False])[earnings_cols]\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                earnings_df.sort_values(by=['symbol', 'period'], ascending=[True, False])[earnings_cols].to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating company earnings data for {ticker_symbol}, error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "824a696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_insider_sentiment(ticker_symbol):\n",
    "    \"\"\" populates the insider sentiment data into the file: 'data/insider_sentiment.csv' \"\"\"\n",
    "\n",
    "    insider_response = get_finhub_client().stock_insider_sentiment(ticker_symbol, \"2013-01-01\", current_year+\"-12-31\")\n",
    "\n",
    "    insider_df = pd.json_normalize(insider_response, record_path='data')\n",
    "\n",
    "    if not insider_df.empty:\n",
    "        csv_file_path = create_csv_path(\"data/insider_sentiment.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, insider_df], axis=0).drop_duplicates().sort_values(by=['symbol','year','month'], ascending=[True,False,False])\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                insider_df.sort_values(by=['symbol','year','month'], ascending=[True,False,False]).to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating insider sentiment data for {ticker_symbol}, error: {error}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce6b1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_columns = ['symbol', 'period', 'strongBuy', 'buy', 'hold', 'sell', 'strongSell']\n",
    "\n",
    "def populate_recommended_trends(ticker_symbol):\n",
    "    \"\"\" populates the latest analyst recommendation trends for a company into the file: 'data/recommendation_trends.csv' \"\"\"\n",
    "\n",
    "    trends = get_finhub_client().recommendation_trends(symbol=ticker_symbol)\n",
    "\n",
    "    trends_df = pd.json_normalize(trends)\n",
    "\n",
    "    if 'symbol' in trends_df and len(trends_df['symbol']) > 0:\n",
    "\n",
    "        trends_df[trends_columns]\n",
    "        csv_file_path = create_csv_path(\"data/recommendation_trends.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, trends_df], axis=0).sort_values(by=['symbol', 'period'], ascending=[True, False]).drop_duplicates(subset=['symbol', 'period'])[trends_columns]\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                trends_df.sort_values(by=['symbol', 'period'], ascending=[True, False]).to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while updating recommendation trends data for {ticker_symbol}, error: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42125235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_senate_lobbying(ticker_symbol):\n",
    "    \"\"\" populates the reported lobbying activities in the Senate and the House into the file: 'data/senate_lobbying.csv' \"\"\"\n",
    "\n",
    "    lobby_response = get_finhub_client().stock_lobbying(ticker_symbol, \"2000-01-01\", today_date)\n",
    "\n",
    "    lobby_df = pd.json_normalize(lobby_response, record_path='data')\n",
    "\n",
    "    if not lobby_df.empty:\n",
    "        csv_file_path = create_csv_path(\"data/senate_lobbying.csv\")\n",
    "\n",
    "        symbol_col = lobby_df.pop('symbol')\n",
    "        year_col = lobby_df.pop('year')\n",
    "        lobby_df.insert(0, 'symbol', symbol_col)\n",
    "        lobby_df.insert(0, 'year', year_col)\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, lobby_df], axis=0).sort_values(by=['symbol', 'year'], ascending=[True, False]).drop_duplicates().dropna(how='all', axis=1)\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                lobby_df.dropna(how='all', axis=1).to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating senate lobbying data for {ticker_symbol}, error: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31c3f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_generate_data_for_ticker(stock_info, stock_tickers):\n",
    "    return 'symbol' in stock_info and len(stock_info['symbol']) > 0 and stock_info['symbol'] in stock_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c295718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Wait... Generating Financial Data. Interrupt the Program to Exit\n",
      "5 Symbols Processed. Current Run Time: 5.020374059677124 seconds\n",
      "10 Symbols Processed. Current Run Time: 7.435729026794434 seconds\n",
      "15 Symbols Processed. Current Run Time: 10.480255126953125 seconds\n",
      "20 Symbols Processed. Current Run Time: 12.655170917510986 seconds\n",
      "25 Symbols Processed. Current Run Time: 14.660192966461182 seconds\n",
      "30 Symbols Processed. Current Run Time: 16.241300106048584 seconds\n",
      "35 Symbols Processed. Current Run Time: 20.56233525276184 seconds\n",
      "40 Symbols Processed. Current Run Time: 23.230959177017212 seconds\n",
      "45 Symbols Processed. Current Run Time: 24.93332004547119 seconds\n",
      "50 Symbols Processed. Current Run Time: 27.40036916732788 seconds\n",
      "55 Symbols Processed. Current Run Time: 30.35839605331421 seconds\n",
      "60 Symbols Processed. Current Run Time: 32.93816113471985 seconds\n",
      "65 Symbols Processed. Current Run Time: 35.846702098846436 seconds\n",
      "70 Symbols Processed. Current Run Time: 39.397249937057495 seconds\n",
      "75 Symbols Processed. Current Run Time: 42.55234503746033 seconds\n",
      "-- Exiting Program -- Total Execution Time: 0.7092112859090169 minutes\n",
      "75 Stock Symbols Processed. Symbol List: ['CRM', 'VZ', 'CZR', 'WYNN', 'UBER', 'ROKU', 'XOM', 'AMC', 'MRK', 'KO', 'INTC', 'MA', 'PFE', 'F', 'TLSA', 'NFLX', 'MANU', 'MTN', 'WFC', 'RCI', 'ANGI', 'CVX', 'NVDA', 'LBRDA', 'MSGS', 'SPOT', 'WOW', 'HD', 'ATUS', 'WWE', 'BMBL', 'LUMN', 'WBD', 'EA', 'PEP', 'EFX', 'CHDN', 'PARA', 'DISH', 'SNAP', 'TVTV', 'FUBO', 'SOFI', 'DKNG', 'META', 'AMZN', 'YELP', 'DG', 'CABO', 'T', 'LVS', 'MTCH', 'NWSA', 'MSFT', 'BYD', 'CMCSA', 'GOOG', 'SIRI', 'CHTR', 'LYFT', 'LLY', 'JNJ', 'SAVE', 'PG', 'PINS', 'UNH', 'NKE', 'AAPL', 'PENN', 'MGM', 'DIS', 'BAC', 'AMD', 'GME', 'V']\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded set of symbols to generate data. Includes Comcast competitors and companies from the sectors listed below:\n",
    "# Ride Share, Gambling, Sports Entertainmnet, Big Tech, Social Media\n",
    "stock_tickers = {'AAPL','AMZN','AMC','AMD','ANGI','ATUS','BAC','BETZ','BMBL','BYD','CABO','CHDN','CHTR','CMCSA','CRM','CVX','CZR','DIS','DISH','DKNG','DG','EA','EFX',\n",
    "'F','FUBO','FWONA','GOOG','GME','HD','INTC','JNJ','JPN','KO','LBRDA','LUMN','LLY','LSXMA','LVS','LYFT','MANU','META','MA','MGM','MRK','MSFT','MSGS','MTCH','MTN',\n",
    "'NFLX','NKE','NVDA','NWSA','PARA','PENN','PEP','PDYPY','PFE','PG','PINS','RCI','ROKU','SAVE','SIRI','SNAP','SOFI','SPOT','T','TLSA','TVTV','TWTR','UBER','UNH',\n",
    "'V','VZ','WBD','WFC','WOW','WWE','WYNN','XOM','YELP'}\n",
    "\n",
    "all_stock_info = get_finhub_client().stock_symbols(exchange=\"US\", currency=\"USD\", security_type=\"Common Stock\")\n",
    "processed_symbols = []\n",
    "\n",
    "print(\"Please Wait... Generating Financial Data. Interrupt the Program to Exit\")\n",
    "\n",
    "for stock_info in all_stock_info:\n",
    "\n",
    "    if should_generate_data_for_ticker(stock_info, stock_tickers):\n",
    "        ticker_symbol = stock_info['symbol']\n",
    "\n",
    "        try:\n",
    "\n",
    "            populate_candlestick_data(ticker_symbol)\n",
    "\n",
    "            populate_technical_data(ticker_symbol)\n",
    "\n",
    "            populate_social_sentiment(ticker_symbol)\n",
    "\n",
    "            populate_insider_transactions(ticker_symbol)\n",
    "\n",
    "            #populate_insider_sentiment(ticker_symbol)\n",
    "\n",
    "            #populate_recommended_trends(ticker_symbol)\n",
    "\n",
    "            #populate_company_surprise_earnings(ticker_symbol)\n",
    "\n",
    "            #populate_senate_lobbying(ticker_symbol)\n",
    "\n",
    "            processed_symbols.append(ticker_symbol)\n",
    "\n",
    "            if len(processed_symbols) % 5 == 0:\n",
    "                print(f\"{len(processed_symbols)} Symbols Processed. Current Run Time: {(time.time() - start_time)} seconds\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while generating data for {ticker_symbol}, error: {e}\")\n",
    "            time.sleep(5)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Process Interrupted\")\n",
    "            break\n",
    "\n",
    "print(f\"-- Exiting Program -- Total Execution Time: {(time.time() - start_time) / 60} minutes\")\n",
    "print(f\"{len(processed_symbols)} Stock Symbols Processed. Symbol List: {processed_symbols}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
